\section{Method} \label{sec:method}

We investigate several different ways to evaluate the observation model.

\subsection{Ray-Based}

In \cite{ushani_raybased}, we extensively studied how to evaluate the
observation by using a ray-based approach. Each observation $z_i$ is modeled as
a ray from the sensor origin to the point return (thus capturing the free space
between the sensor and the returned point). We built a ray-based observation
model for each object class using a discretized lookup table of histograms that
were learned during training. Additionally, to improve performance when dealing
with multiple classes, the conditional dependence of $z_i$ on $z_{i-1}$ was
modeled as well. While this approach showed promising results in a 2D simulated
world, it was intractable to scale up to 3D, with the chief problem being
training a higher dimensional observation model.

\subsection{Occupancy Grid}

To make the problem more tractable in 3D, we considered building an occupancy
grid from the \ac{LIDAR} observations $\mathbf{Z}$. This occupancy grid consists
of a sparse set of voxels $\mathbf{V} = \{v_{1:n_v}\}$, where each voxel $v_i$
is labeled as either free or occupied. Any voxels not contained in this sparse
set is considered to be unknown. We compute $\mathbf{V}$ by performing
ray-casting using Bresenham's ray tracing algorithm
\cite{bresenham1965algorithm}. This can be efficiently implemented on a \ac{GPU}

Thus, for our observation model, we now wish to evaluate:
%
\begin{align}
  p(\mathbf{V} | \mathrm{obj_{c, x}}) \text{.} \label{eq:detection_map}
\end{align}

\subsubsection{Na\"ive Bayes} \label{sec:naive_bayes}

One approach is to make the independence assumption:
%
\begin{align}
  p(\mathbf{V} | \mathrm{obj_{c, x}}) &= \prod_i p(v_i | \mathrm{obj_{c, x}})
  \text{.}
\end{align}
%
The key advantage of this approach is its simplicity and speed.
However, we find that this assumption leads to significant confusion between
classes. As each voxel is assumed to be independent given the presence of an
object, this model does not capture any of the dependencies between voxels in
the case in intra-class variation, leading to degradation in performance. For
example, this model cannot easily distinguish between a car and a bush that is
roughly the same size as a car.

\subsubsection{Chow-Liu Tree Model} \label{sec:clt}

Similar to our work in \cite{ushani_raybased}, we propose better approximating
the full joint distribution $p(\mathbf{V} | \mathrm{obj_{c,x}})$ by modeling
some of the conditional dependence between observations. To do so, we leverage a
\ac{CLT} \citep{chow1968approximating}.

For the observations $\mathbf{V} = \{v_1, \ldots, v_{n_z}\}$, a \ac{CLT} finds a
first order dependency tree between them that minimizes the \ac{KLD} between the
approximated distribution $P$ and the true full joint distribution $Q$:
%
\begin{align}
  D(P || Q) &= -\sum I(v_i, v_{p(i)}) + \sum H(v_i) - H(\mathbf{V})
  \label{eq:kld}
  \text{,}
\end{align}
%
where $I(v_i, v_{p(i)})$ is the mutual information between $v_i$ and its parent
in the tree $v_{p(i)}$, $H(v_i)$ is the entropy of $v_i$, and $H(\mathbf{V})$ is
the joint entropy of $\{v_1, \ldots, v_{n_z}\}$. To minimize the \ac{KLD}, the
\ac{CLT} finds the spanning tree amongst $\mathbf{V}$ that maximizes the total
pairwise mutual information, as the latter two terms in \eqref{eq:kld} are
independent of the dependency tree's structure.

To evaluate the \ac{CLT} approximation of the distribution, we compute:
%
\begin{align}
  p(\mathbf{V)} &\approx p_{clt}(\mathbf{V}) = p(v_r) \prod p(v_i | v_{p(i)})
\end{align}
%
where $v_r$ is the voxel chosen as the root of the spanning tree.

\subsubsection{Greedy CLT Approximation} \label{sec:greedy_clt}

The \ac{CLT} does better at approximating the true distribution than the na\"ive
approach. However, it can prove costly to use at runtime. Since the set of
voxels $\mathbf{V}$ is continually changing, we cannot simply build the \ac{CLT}
offline to evaluate at runtime. We could rebuild the \ac{CLT} every time we need
to use it, but that requires solving the maximum spanning tree for each
evaluation and would still prove costly. Updating the \ac{CLT} or marginalizing
out voxels that are unobserved quickly becomes intractable as well.

We can comprise between approximating the joint distribution and runtime by
taking a greedy approach. Consider some some of voxel observations $\mathbf{V}$
that we wish to evaluate. We choose the first $v_1$ to be the root of the tree.
Next, for every following $v_i$ we find the preceding voxel observation $v_j, j
< i$ that maximizes $I(v_i, v_j)$ and add this edge to the tree. As voxels tend
to have more mutual information with other voxels that are nearby, we can
further save on runtime by only searching for $j > i - n_v$, where $n_v$ is a
constant, where we take care to process $\mathbf{V}$ in spatial order.

\subsubsection{Surface Normals} \label{sec:normals}

We can augment the occupancy grid by computing surface normals. For any voxel
$v_i$ that is occupied and thus contains some \ac{LIDAR} points $\{z_j\}$, we
can approximate the surface captured by these points and compute a surface
normal. We do this by computing the covariance $\mat{C}$ of all points within
$v$ and finding the eigenvector of $\mat{C}$ corresponding to the smallest
eigenvector. These surface normals can then be used to augment
any of the above approaches, although we specifically considered it with
\secref{sec:naive_bayes}. For example, instead of evaluating just
%
\begin{align}
  p(v_i \text{ is occupied } | \mathrm{obj_{c,x}}) \text{,}
\end{align}
%
as we would in \secref{sec:naive_bayes}, we now evaluate
%
\begin{align}
  p(v_i \text{ is occupied } | \mathrm{obj_{c,x}}) p(v_i \text{ has normal } \vec{n}
  | \mathrm{obj_{c, x}}) \text{.}
\end{align}

\subsection{Training} \label{sec:training}

All of our models were trained using KITTI data. During the training phase, we
sample observations (i.e., a local occupancy grid) for each class. Once all samples are
extracted, they are then accumulated together to estimate the observation model
for each class. This includes marginal probabilities for
\secref{sec:naive_bayes}, mutual information and conditional probabilities for
\secref{sec:clt} and \secref{sec:greedy_clt}, and discretizied surface normals
for \secref{sec:normals}.

In this work, the model for each object extends extends \unit{6}{\m} in each of $x$ and $y$
and \unit{4}{\m} vertically. A unique model for each is learned for each of 8
orientations from \unit{0} to \unit{360}{\deg}. To compensate for the pose
discretization, we randomly sample several slightly translated and rotated
instances of each object.
